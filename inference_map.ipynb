{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference mmdetection model from other format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inference mmdetection pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/yolov3_mobilenetv2_pretrained/best_pascal_voc_mAP_epoch_27.pth\n",
      "<DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    ori_shape: (500, 353)\n",
      "    batch_input_shape: (224, 160)\n",
      "    scale_factor: (0.4475920679886686, 0.448)\n",
      "    img_id: 0\n",
      "    img_shape: (224, 158)\n",
      "    pad_shape: (224, 160)\n",
      "    img_path: None\n",
      "\n",
      "    DATA FIELDS\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([], size=(0, 4))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "        ) at 0x7f7c64b7f700>\n",
      "    pred_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([[  1.4124,   2.3510, 347.7869, 496.0396],\n",
      "                        [ 63.7587, 245.1837, 171.3031, 365.1945],\n",
      "                        [ 63.7587, 245.1837, 171.3031, 365.1945]])\n",
      "            scores: tensor([0.9992, 0.6674, 0.3348])\n",
      "            labels: tensor([14, 11,  7])\n",
      "        ) at 0x7f7c64b7f7c0>\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([], size=(0, 4))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "        ) at 0x7f7c64b7f670>\n",
      ") at 0x7f7c64b7f760>\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, async_inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import xmltodict\n",
    "from mmengine.fileio import list_from_file\n",
    "\n",
    "# 指定模型的配置文件和 checkpoint 文件路径\n",
    "config_file = 'configs/yolo/yolov3_mobilenetv2_8xb24-ms-416-300e_coco.py'\n",
    "checkpoint_file = 'work_dirs/yolov3_mobilenetv2_pretrained/best_pascal_voc_mAP_epoch_27.pth'\n",
    "class_dict = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 1,\n",
    "    'bird': 2,\n",
    "    'boat': 3,\n",
    "    'bottle': 4,\n",
    "    'bus': 5,\n",
    "    'car': 6,\n",
    "    'cat': 7,\n",
    "    'chair': 8,\n",
    "    'cow': 9,\n",
    "    'diningtable': 10,\n",
    "    'dog': 11,\n",
    "    'horse': 12,\n",
    "    'motorbike': 13,\n",
    "    'person': 14,\n",
    "    'pottedplant': 15,\n",
    "    'sheep': 16,\n",
    "    'sofa': 17,\n",
    "    'train': 18,\n",
    "    'tvmonitor': 19\n",
    "}\n",
    "\n",
    "#Register all modules in mmdet into the registries\n",
    "register_all_modules()\n",
    "# 若检测到有GPU则使用GPU\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 根据配置文件和 checkpoint 文件构建模型\n",
    "model = init_detector(config_file, checkpoint_file, device=device)\n",
    "model.eval()\n",
    "\n",
    "# 指定要遍历的文件夹路径\n",
    "img_ids = list_from_file(\"../data/VOCdevkit/VOC2007/ImageSets/Main/test.txt\")\n",
    "\n",
    "det_results, annotations = [], []\n",
    "# 遍历所有的jpg文件\n",
    "for img_id in img_ids:\n",
    "\n",
    "    img_path = os.path.join('../data/VOCdevkit/VOC2007/JPEGImages/', img_id + '.jpg')\n",
    "    # 获取预测结果\n",
    "    img = mmcv.imread(img_path, channel_order='rgb')\n",
    "    output2 = inference_detector(model, img)\n",
    "\n",
    "    pred_bboxes = output2.pred_instances.bboxes.cpu().numpy()\n",
    "    pred_scores = output2.pred_instances.scores.cpu().numpy()\n",
    "    pred_labels = output2.pred_instances.labels.cpu().numpy()\n",
    "    dets = []\n",
    "    for label in range(len(class_dict)):\n",
    "        index = np.where(pred_labels == label)[0]\n",
    "        pred_bbox_scores = np.hstack(\n",
    "            [pred_bboxes[index], pred_scores[index].reshape((-1, 1))])\n",
    "        dets.append(pred_bbox_scores)\n",
    "    \n",
    "    det_results.append(dets)\n",
    "    \n",
    "    # 解析XML文件\n",
    "    ann_path = os.path.join('../data/VOCdevkit/VOC2007/Annotations/', img_id + '.xml')\n",
    "    with open(ann_path) as f:\n",
    "        xml_data = xmltodict.parse(f.read())\n",
    "    bboxes, labels = [], []\n",
    "    bboxes_ignore, labels_ignore = [], []\n",
    "    obj = xml_data['annotation']['object']\n",
    "    if type(obj) == list:\n",
    "        for i in range(len(obj)):\n",
    "            if obj[i]['difficult'] == '0':\n",
    "                bboxes.append([int(obj[i]['bndbox']['xmin'])-1, int(obj[i]['bndbox']['ymin'])-1, \n",
    "                            int(obj[i]['bndbox']['xmax'])-1, int(obj[i]['bndbox']['ymax'])-1])\n",
    "                labels.append(class_dict[obj[i]['name']])\n",
    "            else:\n",
    "                bboxes_ignore.append([int(obj[i]['bndbox']['xmin'])-1, int(obj[i]['bndbox']['ymin'])-1, \n",
    "                            int(obj[i]['bndbox']['xmax'])-1, int(obj[i]['bndbox']['ymax'])-1])\n",
    "                labels_ignore.append(class_dict[obj[i]['name']])\n",
    "    else:\n",
    "        if obj['difficult'] == '0':\n",
    "            bboxes.append([int(obj['bndbox']['xmin'])-1, int(obj['bndbox']['ymin'])-1, \n",
    "                        int(obj['bndbox']['xmax'])-1, int(obj['bndbox']['ymax'])-1])\n",
    "            labels.append(class_dict[obj['name']])\n",
    "        else:\n",
    "            bboxes_ignore.append([int(obj['bndbox']['xmin'])-1, int(obj['bndbox']['ymin'])-1, \n",
    "                        int(obj['bndbox']['xmax'])-1, int(obj['bndbox']['ymax'])-1])\n",
    "            labels_ignore.append(class_dict[obj['name']])\n",
    "\n",
    "    bboxes = torch.tensor(bboxes).cpu().numpy().astype(np.float32)\n",
    "    labels = torch.tensor(labels).cpu().numpy()\n",
    "    bboxes_ignore = torch.tensor(bboxes_ignore).cpu().numpy().astype(np.float32)\n",
    "    labels_ignore = torch.tensor(labels_ignore).cpu().numpy()\n",
    "\n",
    "    ann = {'bboxes': bboxes,\n",
    "           'labels': labels,\n",
    "           'bboxes_ignore': torch.empty(size=(0,4)).cpu().numpy() if len(bboxes_ignore) == 0 else bboxes_ignore,\n",
    "           'labels_ignore': torch.empty(dtype=torch.int64, size=(0,)).cpu().numpy() if len(labels_ignore) == 0 else labels_ignore}\n",
    "    annotations.append(ann)\n",
    "\n",
    "print(det_results)\n",
    "print(annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': array([[ 47., 239., 194., 370.],\n",
       "        [  7.,  11., 351., 497.]], dtype=float32),\n",
       " 'labels': array([11, 14]),\n",
       " 'bboxes_ignore': array([], shape=(0, 4), dtype=float32),\n",
       " 'labels_ignore': array([], dtype=int64)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation preds with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------+------+------+--------+-------+\n",
      "| class | gts  | dets | recall | ap    |\n",
      "+-------+------+------+--------+-------+\n",
      "| 0     | 285  | 705  | 0.582  | 0.496 |\n",
      "| 1     | 337  | 1842 | 0.748  | 0.613 |\n",
      "| 2     | 459  | 720  | 0.497  | 0.399 |\n",
      "| 3     | 263  | 1654 | 0.517  | 0.319 |\n",
      "| 4     | 469  | 1205 | 0.260  | 0.208 |\n",
      "| 5     | 213  | 712  | 0.732  | 0.591 |\n",
      "| 6     | 1201 | 2840 | 0.649  | 0.566 |\n",
      "| 7     | 358  | 773  | 0.696  | 0.587 |\n",
      "| 8     | 756  | 3625 | 0.549  | 0.311 |\n",
      "| 9     | 244  | 477  | 0.537  | 0.404 |\n",
      "| 10    | 206  | 1486 | 0.718  | 0.491 |\n",
      "| 11    | 489  | 1325 | 0.714  | 0.577 |\n",
      "| 12    | 348  | 899  | 0.721  | 0.591 |\n",
      "| 13    | 325  | 1460 | 0.806  | 0.690 |\n",
      "| 14    | 4528 | 8503 | 0.624  | 0.533 |\n",
      "| 15    | 480  | 1324 | 0.448  | 0.315 |\n",
      "| 16    | 242  | 511  | 0.550  | 0.435 |\n",
      "| 17    | 239  | 1156 | 0.770  | 0.521 |\n",
      "| 18    | 282  | 750  | 0.716  | 0.638 |\n",
      "| 19    | 308  | 1147 | 0.630  | 0.482 |\n",
      "+-------+------+------+--------+-------+\n",
      "| mAP   |      |      |        | 0.488 |\n",
      "+-------+------+------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "from mmdet.evaluation.functional.mean_ap import eval_map\n",
    "\n",
    "mean_ap, eval_results = eval_map(det_results, annotations, eval_mode='11points', use_legacy_coordinate=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
