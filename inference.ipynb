{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference mmdetection model from other format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inference mmdetection pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/yolov3_mobilenetv2_pretrained/best_pascal_voc_mAP_epoch_27.pth\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m image_tensor \u001b[39m=\u001b[39m image_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m output \u001b[39m=\u001b[39m model(image_tensor)\n\u001b[0;32m---> 62\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mbbox_head\u001b[39m.\u001b[39;49mpredict_by_feat(output,batch_img_metas\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     63\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[1;32m     64\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/lsj/mmdetection/mmdet/models/dense_heads/yolo_head.py:229\u001b[0m, in \u001b[0;36mYOLOV3Head.predict_by_feat\u001b[0;34m(self, pred_maps, batch_img_metas, cfg, rescale, with_nms)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_by_feat\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    197\u001b[0m                     pred_maps: Sequence[Tensor],\n\u001b[1;32m    198\u001b[0m                     batch_img_metas: Optional[List[\u001b[39mdict\u001b[39m]],\n\u001b[1;32m    199\u001b[0m                     cfg: OptConfigType \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m                     rescale: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m                     with_nms: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InstanceList:\n\u001b[1;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transform a batch of output features extracted from the head into\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    bbox results. It has been accelerated since PR #5991.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m          the last dimension 4 arrange as (x1, y1, x2, y2).\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(pred_maps) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_levels\n\u001b[1;32m    230\u001b[0m     cfg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_cfg \u001b[39mif\u001b[39;00m cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m cfg\n\u001b[1;32m    231\u001b[0m     cfg \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(cfg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, async_inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import xmltodict\n",
    "\n",
    "# 指定模型的配置文件和 checkpoint 文件路径\n",
    "config_file = 'configs/yolo/yolov3_mobilenetv2_8xb24-ms-416-300e_coco.py'\n",
    "checkpoint_file = 'work_dirs/yolov3_mobilenetv2_pretrained/best_pascal_voc_mAP_epoch_27.pth'\n",
    "class_dict = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 1,\n",
    "    'bird': 2,\n",
    "    'boat': 3,\n",
    "    'bottle': 4,\n",
    "    'bus': 5,\n",
    "    'car': 6,\n",
    "    'cat': 7,\n",
    "    'chair': 8,\n",
    "    'cow': 9,\n",
    "    'diningtable': 10,\n",
    "    'dog': 11,\n",
    "    'horse': 12,\n",
    "    'motorbike': 13,\n",
    "    'person': 14,\n",
    "    'pottedplant': 15,\n",
    "    'sheep': 16,\n",
    "    'sofa': 17,\n",
    "    'train': 18,\n",
    "    'tvmonitor': 19\n",
    "}\n",
    "\n",
    "#Register all modules in mmdet into the registries\n",
    "register_all_modules()\n",
    "# 若检测到有GPU则使用GPU\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 根据配置文件和 checkpoint 文件构建模型\n",
    "model = init_detector(config_file, checkpoint_file, device=device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 定义预处理方法\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_sample = []\n",
    "# 加载图像并进行预处理\n",
    "image = Image.open('../data/VOCdevkit/VOC2007/test_img/000001.jpg')\n",
    "image_tensor = preprocess(image)\n",
    "# 添加一个维度以匹配模型的输入\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "output = model(image_tensor)\n",
    "results = model.bbox_head.predict_by_feat(output,batch_img_metas=None)\n",
    "print(output)\n",
    "break\n",
    "# 指定要遍历的文件夹路径\n",
    "folder_path = \"../data/VOCdevkit/VOC2007/test_img/\"\n",
    "# 使用glob模块匹配文件夹下所有的jpg文件\n",
    "jpg_files = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "\n",
    "data_samples = []\n",
    "# 遍历所有的jpg文件\n",
    "for file_path in jpg_files:\n",
    "\n",
    "    data_sample = {}\n",
    "    # 获取预测结果\n",
    "    img = mmcv.imread(file_path, channel_order='rgb')\n",
    "    output2 = inference_detector(model, img)\n",
    "\n",
    "    data_sample.update({\n",
    "        'pred_instances': {\n",
    "            'bboxes': output2.pred_instances.bboxes,\n",
    "            'labels': output2.pred_instances.labels,\n",
    "            'scores': output2.pred_instances.scores\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # 解析XML文件\n",
    "    file_name = os.path.basename(file_path)\n",
    "    xml_file = os.path.join('../data/VOCdevkit/VOC2007/Annotations/', file_name[:-4] + '.xml')\n",
    "    with open(xml_file) as f:\n",
    "        xml_data = xmltodict.parse(f.read())\n",
    "\n",
    "    bboxes, labels = [], []\n",
    "    obj = xml_data['annotation']['object']\n",
    "    if type(obj) == list:\n",
    "        for i in range(len(obj)):\n",
    "            bboxes.append([int(obj[i]['bndbox']['xmin']), int(obj[i]['bndbox']['ymin']), \n",
    "                           int(obj[i]['bndbox']['xmax']), int(obj[i]['bndbox']['ymax'])])\n",
    "            labels.append(class_dict[obj[i]['name']])\n",
    "    else:\n",
    "        bboxes.append([int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), \n",
    "                       int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])])\n",
    "        labels.append(class_dict[obj['name']])\n",
    "\n",
    "    bboxes = torch.tensor(bboxes)\n",
    "    labels = torch.tensor(labels)\n",
    "    data_sample.update({\n",
    "        'gt_instances': {\n",
    "            'bboxes': bboxes,\n",
    "            'labels': labels\n",
    "        },\n",
    "\n",
    "        'ignored_instances': {\n",
    "            'bboxes': torch.empty(size=(0,4)),\n",
    "            'labels': torch.empty(dtype=torch.int64, size=(0,))\n",
    "        }\n",
    "    })\n",
    "\n",
    "    data_samples.append(data_sample)\n",
    "\n",
    "print(len(data_samples))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation preds with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']}\n",
      "05/25 10:58:47 - mmengine - INFO - \n",
      "---------------iou_thr: 0.5---------------\n",
      "05/25 10:58:48 - mmengine - INFO - \n",
      "+-------------+------+------+--------+-------+\n",
      "| class       | gts  | dets | recall | ap    |\n",
      "+-------------+------+------+--------+-------+\n",
      "| aeroplane   | 311  | 705  | 0.537  | 0.475 |\n",
      "| bicycle     | 389  | 1842 | 0.663  | 0.537 |\n",
      "| bird        | 576  | 720  | 0.398  | 0.321 |\n",
      "| boat        | 393  | 1654 | 0.361  | 0.233 |\n",
      "| bottle      | 657  | 1205 | 0.192  | 0.156 |\n",
      "| bus         | 254  | 712  | 0.630  | 0.516 |\n",
      "| car         | 1541 | 2840 | 0.517  | 0.466 |\n",
      "| cat         | 370  | 773  | 0.678  | 0.583 |\n",
      "| chair       | 1374 | 3625 | 0.370  | 0.222 |\n",
      "| cow         | 329  | 477  | 0.422  | 0.325 |\n",
      "| diningtable | 299  | 1486 | 0.572  | 0.385 |\n",
      "| dog         | 530  | 1325 | 0.666  | 0.533 |\n",
      "| horse       | 395  | 899  | 0.643  | 0.528 |\n",
      "| motorbike   | 369  | 1460 | 0.726  | 0.619 |\n",
      "| person      | 5227 | 8503 | 0.551  | 0.467 |\n",
      "| pottedplant | 592  | 1324 | 0.387  | 0.277 |\n",
      "| sheep       | 311  | 511  | 0.447  | 0.359 |\n",
      "| sofa        | 396  | 1156 | 0.621  | 0.427 |\n",
      "| train       | 302  | 750  | 0.685  | 0.587 |\n",
      "| tvmonitor   | 361  | 1147 | 0.562  | 0.427 |\n",
      "+-------------+------+------+--------+-------+\n",
      "| mAP         |      |      |        | 0.422 |\n",
      "+-------------+------+------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "from mmengine.evaluator import Evaluator\n",
    "from mmengine.fileio import load\n",
    "\n",
    "a  = {'classes': ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable',\n",
    "                  'dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']}\n",
    "\n",
    "# 构建评测器。参数 `metrics` 为评测指标配置\n",
    "evaluator = Evaluator(metrics=dict(type='VOCMetric', metric='mAP', eval_mode='11points'))\n",
    "evaluator.dataset_meta = a\n",
    "print(evaluator.dataset_meta)\n",
    "\n",
    "# 调用评测器离线评测接口，得到评测结果\n",
    "# chunk_size 表示每次处理的样本数量，可根据内存大小调整\n",
    "results = evaluator.offline_evaluate(data_samples, chunk_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
