{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference mmdetection model from other format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inference mmdetection pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/yolov3_mobilenetv2_pretrained/best_pascal_voc_mAP_epoch_27.pth\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m image \u001b[39m=\u001b[39m mmcv\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39m../data/VOCdevkit/VOC2007/test_img/000001.jpg\u001b[39m\u001b[39m'\u001b[39m, channel_order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(image))\n\u001b[0;32m---> 59\u001b[0m image_tensor \u001b[39m=\u001b[39m preprocess(image)\n\u001b[1;32m     60\u001b[0m \u001b[39m# 添加一个维度以匹配模型的输入\u001b[39;00m\n\u001b[1;32m     61\u001b[0m image_tensor \u001b[39m=\u001b[39m image_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[39m=\u001b[39m [size]\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mget_dimensions(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, async_inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import xmltodict\n",
    "\n",
    "# 指定模型的配置文件和 checkpoint 文件路径\n",
    "config_file = 'configs/yolo/yolov3_mobilenetv2_8xb24-ms-416-300e_coco.py'\n",
    "checkpoint_file = 'work_dirs/yolov3_mobilenetv2_pretrained/best_pascal_voc_mAP_epoch_27.pth'\n",
    "class_dict = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 1,\n",
    "    'bird': 2,\n",
    "    'boat': 3,\n",
    "    'bottle': 4,\n",
    "    'bus': 5,\n",
    "    'car': 6,\n",
    "    'cat': 7,\n",
    "    'chair': 8,\n",
    "    'cow': 9,\n",
    "    'diningtable': 10,\n",
    "    'dog': 11,\n",
    "    'horse': 12,\n",
    "    'motorbike': 13,\n",
    "    'person': 14,\n",
    "    'pottedplant': 15,\n",
    "    'sheep': 16,\n",
    "    'sofa': 17,\n",
    "    'train': 18,\n",
    "    'tvmonitor': 19\n",
    "}\n",
    "\n",
    "#Register all modules in mmdet into the registries\n",
    "register_all_modules()\n",
    "# 若检测到有GPU则使用GPU\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 根据配置文件和 checkpoint 文件构建模型\n",
    "model = init_detector(config_file, checkpoint_file, device=device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 定义预处理方法\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_sample = []\n",
    "# 加载图像并进行预处理\n",
    "image = Image.open('../data/VOCdevkit/VOC2007/test_img/000001.jpg')\n",
    "image_tensor = preprocess(image)\n",
    "# 添加一个维度以匹配模型的输入\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "output = model(image_tensor)\n",
    "batch_img_metas = ['../data/VOCdevkit/VOC2007/test_img/000001.jpg']\n",
    "results = model.bbox_head.predict_by_feat(*output, batch_img_metas=batch_img_metas)\n",
    "print(len(output))\n",
    "break\n",
    "# 指定要遍历的文件夹路径\n",
    "folder_path = \"../data/VOCdevkit/VOC2007/test_img/\"\n",
    "# 使用glob模块匹配文件夹下所有的jpg文件\n",
    "jpg_files = glob.glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "\n",
    "data_samples = []\n",
    "# 遍历所有的jpg文件\n",
    "for file_path in jpg_files:\n",
    "\n",
    "    data_sample = {}\n",
    "    # 获取预测结果\n",
    "    img = mmcv.imread(file_path, channel_order='rgb')\n",
    "    output2 = inference_detector(model, img)\n",
    "\n",
    "    data_sample.update({\n",
    "        'pred_instances': {\n",
    "            'bboxes': output2.pred_instances.bboxes,\n",
    "            'labels': output2.pred_instances.labels,\n",
    "            'scores': output2.pred_instances.scores\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # 解析XML文件\n",
    "    file_name = os.path.basename(file_path)\n",
    "    xml_file = os.path.join('../data/VOCdevkit/VOC2007/Annotations/', file_name[:-4] + '.xml')\n",
    "    with open(xml_file) as f:\n",
    "        xml_data = xmltodict.parse(f.read())\n",
    "\n",
    "    bboxes, labels = [], []\n",
    "    obj = xml_data['annotation']['object']\n",
    "    if type(obj) == list:\n",
    "        for i in range(len(obj)):\n",
    "            bboxes.append([int(obj[i]['bndbox']['xmin']), int(obj[i]['bndbox']['ymin']), \n",
    "                           int(obj[i]['bndbox']['xmax']), int(obj[i]['bndbox']['ymax'])])\n",
    "            labels.append(class_dict[obj[i]['name']])\n",
    "    else:\n",
    "        bboxes.append([int(obj['bndbox']['xmin']), int(obj['bndbox']['ymin']), \n",
    "                       int(obj['bndbox']['xmax']), int(obj['bndbox']['ymax'])])\n",
    "        labels.append(class_dict[obj['name']])\n",
    "\n",
    "    bboxes = torch.tensor(bboxes)\n",
    "    labels = torch.tensor(labels)\n",
    "    data_sample.update({\n",
    "        'gt_instances': {\n",
    "            'bboxes': bboxes,\n",
    "            'labels': labels\n",
    "        },\n",
    "\n",
    "        'ignored_instances': {\n",
    "            'bboxes': torch.empty(size=(0,4)),\n",
    "            'labels': torch.empty(dtype=torch.int64, size=(0,))\n",
    "        }\n",
    "    })\n",
    "\n",
    "    data_samples.append(data_sample)\n",
    "\n",
    "print(len(data_samples))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation preds with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']}\n",
      "05/25 10:58:47 - mmengine - INFO - \n",
      "---------------iou_thr: 0.5---------------\n",
      "05/25 10:58:48 - mmengine - INFO - \n",
      "+-------------+------+------+--------+-------+\n",
      "| class       | gts  | dets | recall | ap    |\n",
      "+-------------+------+------+--------+-------+\n",
      "| aeroplane   | 311  | 705  | 0.537  | 0.475 |\n",
      "| bicycle     | 389  | 1842 | 0.663  | 0.537 |\n",
      "| bird        | 576  | 720  | 0.398  | 0.321 |\n",
      "| boat        | 393  | 1654 | 0.361  | 0.233 |\n",
      "| bottle      | 657  | 1205 | 0.192  | 0.156 |\n",
      "| bus         | 254  | 712  | 0.630  | 0.516 |\n",
      "| car         | 1541 | 2840 | 0.517  | 0.466 |\n",
      "| cat         | 370  | 773  | 0.678  | 0.583 |\n",
      "| chair       | 1374 | 3625 | 0.370  | 0.222 |\n",
      "| cow         | 329  | 477  | 0.422  | 0.325 |\n",
      "| diningtable | 299  | 1486 | 0.572  | 0.385 |\n",
      "| dog         | 530  | 1325 | 0.666  | 0.533 |\n",
      "| horse       | 395  | 899  | 0.643  | 0.528 |\n",
      "| motorbike   | 369  | 1460 | 0.726  | 0.619 |\n",
      "| person      | 5227 | 8503 | 0.551  | 0.467 |\n",
      "| pottedplant | 592  | 1324 | 0.387  | 0.277 |\n",
      "| sheep       | 311  | 511  | 0.447  | 0.359 |\n",
      "| sofa        | 396  | 1156 | 0.621  | 0.427 |\n",
      "| train       | 302  | 750  | 0.685  | 0.587 |\n",
      "| tvmonitor   | 361  | 1147 | 0.562  | 0.427 |\n",
      "+-------------+------+------+--------+-------+\n",
      "| mAP         |      |      |        | 0.422 |\n",
      "+-------------+------+------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "from mmengine.evaluator import Evaluator\n",
    "from mmengine.fileio import load\n",
    "\n",
    "a  = {'classes': ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable',\n",
    "                  'dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']}\n",
    "\n",
    "# 构建评测器。参数 `metrics` 为评测指标配置\n",
    "evaluator = Evaluator(metrics=dict(type='VOCMetric', metric='mAP', eval_mode='11points'))\n",
    "evaluator.dataset_meta = a\n",
    "print(evaluator.dataset_meta)\n",
    "\n",
    "# 调用评测器离线评测接口，得到评测结果\n",
    "# chunk_size 表示每次处理的样本数量，可根据内存大小调整\n",
    "results = evaluator.offline_evaluate(data_samples, chunk_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
